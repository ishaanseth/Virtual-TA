# TDS Virtual TA Project

This project implements a Virtual Teaching Assistant for the "Tools in Data Science" course. It uses a FastAPI backend, scrapes course content and Discourse forum posts, generates embeddings, and uses a Large Language Model (LLM) via AI Pipe to answer student questions based on the retrieved context.

## Features

*   Scrapes course content from the official course website.
*   Scrapes relevant posts from the TDS Discourse forum within a specified date range.
*   Generates text embeddings for all scraped content.
*   Provides a FastAPI endpoint (`/api/`) to answer questions.
*   Uses a RAG (Retrieval Augmented Generation) approach:
    *   Finds relevant documents using semantic similarity (cosine similarity on embeddings).
    *   Augments context by fetching subsequent replies for relevant Discourse posts.
    *   Sends the question and retrieved context to an LLM (via AI Pipe) for answer generation.
*   Configured for evaluation with `promptfoo`.

## Project Structure

`/
.
├── main.py # FastAPI application
├── scrapers/
│ ├── new_course.py # Selenium-based course content scraper
│ └── new_discourse.py # Discourse forum scraper
├── course_content.json # Generated by new_course.py (if run)
├── discourse_posts_v2.json # Generated by new_discourse.py (if run)
├── content_embeddings.json # Generated by main.py on first startup (from the above JSONs)
├── requirements.txt # Python dependencies
├── Dockerfile # For containerizing the application (optional for local run)
├── .env.example # Example environment variables needed
├── .env # Local environment variables (DO NOT COMMIT THIS)
├── project-tds-virtual-ta-promptfoo.yaml # Promptfoo evaluation configuration
├── README.md # This file
└── LICENSE # MIT License
/`


## Setup and Installation

1.  **Clone the Repository:**
    ```bash
    git clone <your-repository-url>
    cd <your-repository-name>
    ```

2.  **Create and Activate a Python Virtual Environment:**
    ```bash
    python -m venv .venv
    # On Windows
    .\.venv\Scripts\activate
    # On macOS/Linux
    source .venv/bin/activate
    ```

3.  **Install Dependencies:**
    *   Ensure you have Google Chrome installed (for the Selenium-based course scraper).
    *   Install Python packages:
        ```bash
        pip install -r requirements.txt
        ```
        This will install `fastapi`, `uvicorn`, `numpy`, `scikit-learn`, `python-dotenv`, `openai`, `selenium`, `webdriver-manager`, `beautifulsoup4`, and `requests`.

4.  **Set Up Environment Variables:**
    *   Copy `.env.example` to a new file named `.env`:
        ```bash
        # On Windows
        copy .env.example .env
        # On macOS/Linux
        cp .env.example .env
        ```
    *   Edit the `.env` file and fill in your actual values:
        ```dotenv
        AIPIPE_TOKEN="your_actual_aipipe_token_here"
        OPENAI_API_BASE_FOR_EMBEDDINGS="https://aipipe.org/openai/v1"
        OPENROUTER_API_BASE_FOR_CHAT="https://aipipe.org/openrouter/v1"
        EMBEDDING_MODEL_NAME="text-embedding-3-small"
        CHAT_MODEL_NAME="google/gemini-flash-1.5" # Or your preferred chat model via AI Pipe
        ```
        *   `AIPIPE_TOKEN`: Your personal token for accessing AI Pipe.
        *   The base URLs and model names are examples; adjust if needed based on AI Pipe documentation and desired models.

5.  **Prepare Data (Scraping - Optional if using pre-scraped files):**
    *   The application expects `course_content.json` and `discourse_posts_v2.json` to be present in the root directory for generating embeddings.
    *   If you don't have these files or want to re-scrape:
        *   **Discourse Scraper:**
            *   Edit `scrapers/new_discourse.py` if necessary (e.g., to update cookie strings or date ranges, though the current one is set for the project).
            *   Run: `python scrapers/new_discourse.py`
            *   This will generate `discourse_posts_v2.json`.
        *   **Course Content Scraper (Selenium):**
            *   Run: `python scrapers/new_course.py`
            *   This will generate `course_content.json`. This scraper uses Selenium and will open a headless Chrome browser.

    *   **Note:** If you have pre-scraped `course_content.json` and `discourse_posts_v2.json` files, place them in the project's root directory.

## Running the Application (FastAPI with Uvicorn)

1.  **Activate your virtual environment** (if not already active):
    ```bash
    # On Windows
    .\.venv\Scripts\activate
    # On macOS/Linux
    source .venv/bin/activate
    ```

2.  **Start the FastAPI Server:**
    ```bash
    uvicorn main:app --reload --host 0.0.0.0 --port 8000
    ```
    *   `--reload`: Enables auto-reloading when code changes (useful for development).
    *   `--host 0.0.0.0`: Makes the server accessible on your network (and for tools like ngrok).
    *   `--port 8000`: Runs on port 8000.

3.  **First Run - Embedding Generation:**
    *   The very first time you run `main.py` (or if `content_embeddings.json` is deleted), it will generate embeddings for all content in `course_content.json` and `discourse_posts_v2.json`. This process can take a significant amount of time depending on the data size and API responsiveness.
    *   Subsequent startups will be much faster as it will load pre-computed embeddings from `content_embeddings.json`.
    *   Monitor the console output for progress.

4.  The API will be available at `http://127.0.0.1:8000` (or `http://localhost:8000`). The specific endpoint for queries is `http://127.0.0.1:8000/api/`.

## Sending Queries to the API

The API endpoint is `/api/` and accepts `POST` requests with a JSON body.

**Example using `curl` (for bash/zsh - adjust for PowerShell):**
```bash
curl -X POST -H "Content-Type: application/json" \
-d '{"question": "What are development tools?"}' \
http://127.0.0.1:8000/api/
```

**Example using PowerShell's Invoke-RestMethod:**

```bash
Invoke-RestMethod -Uri http://127.0.0.1:8000/api/ -Method Post -ContentType "application/json" -Body '{"question": "What are development tools?"}'
```

Expected Request Body Format:

Generated json
{
  "question": "Your question string here",
  "image": "base64_encoded_image_string_or_null" 
}
Use code with caution.
Json
The image field is optional.

Expected Response Body Format:

Generated json
{
  "answer": "The LLM-generated answer based on context.",
  "links": [
    {
      "url": "source_document_url_1",
      "text": "Title or description of source_document_1"
    },
    {
      "url": "source_document_url_2",
      "text": "Title or description of source_document_2"
    }
  ]
}
Use code with caution.
Json
Running Evaluations with promptfoo
Ensure Node.js and npx are installed.

Set up environment variables for promptfoo's LLM grader:
promptfoo uses its own LLM instance (via AI Pipe in this setup) to evaluate the quality of your API's answers using llm-rubric.

Generated powershell
# In PowerShell
$env:AIPIPE_TOKEN="your_actual_aipipe_token" 
$env:OPENAI_API_KEY=$env:AIPIPE_TOKEN       
$env:OPENAI_BASE_URL="https://aipipe.org/openai/v1"
Use code with caution.
Powershell
Generated bash
# In bash/zsh
export AIPIPE_TOKEN="your_actual_aipipe_token"
export OPENAI_API_KEY=$AIPIPE_TOKEN
export OPENAI_BASE_URL="https://aipipe.org/openai/v1"
Use code with caution.
Bash
Run the evaluation:
Make sure your FastAPI server is running locally.

Generated bash
npx -y promptfoo eval --config project-tds-virtual-ta-promptfoo.yaml --no-cache
Use code with caution.
Bash
--no-cache ensures promptfoo calls your API fresh for each test, not using its own cache.

View Results:

Generated bash
npx -y promptfoo view
Use code with caution.
Bash
This will open a web interface to inspect the evaluation results.

Deployment (Example: Using ngrok for Temporary Public URL)
For quick testing or if allowed for submission:

Ensure your local FastAPI server is running (e.g., uvicorn main:app --host 0.0.0.0 --port 8000).

Download ngrok from ngrok.com.

Authenticate ngrok (optional but recommended): .\ngrok.exe authtoken YOUR_NGROK_AUTHTOKEN

Expose your local server:

Generated bash
.\ngrok.exe http 8000
Use code with caution.
Bash
ngrok will provide a public HTTPS URL (e.g., https://random-string.ngrok-free.app). Use this URL (appended with /api/) for your promptfoo.yaml or for submission.
Note: You must keep your local server and the ngrok process running for the URL to be accessible.

For more permanent deployments, consider platforms like Google Cloud Run, PythonAnywhere, etc., which would typically involve containerizing the application with the provided Dockerfile.

Project Structure for GitHub Submission
Include: main.py, scrapers/, requirements.txt, Dockerfile, .env.example, project-tds-virtual-ta-promptfoo.yaml, README.md, LICENSE.

Exclude (using .gitignore): .env, content_embeddings.json, full course_content.json and discourse_posts_v2.json (if very large), .venv/, __pycache__/.

You may include small samples of the data JSONs if desired for context.

Troubleshooting
Embedding Generation Failure: Ensure AIPIPE_TOKEN is correct and has quota. Check for errors from the openai library.

promptfoo Errors:

OPENAI_API_KEY not set: Ensure you've set the environment variables OPENAI_API_KEY (to your AI Pipe token) and OPENAI_BASE_URL in the terminal where you run promptfoo.

File not found for variable image: Make sure project-tds-virtual-ta-q1.webp is in the root of your project directory.

CORS Errors (when calling deployed API from a browser/promptfoo): Ensure CORSMiddleware is correctly configured in main.py.

Cloud Run Deployment Issues: Check Cloud Run logs for detailed error messages. Common issues include incorrect Dockerfile setup, missing dependencies, insufficient memory/CPU, or incorrect environment variable configuration in the Cloud Run service.

Generated code
**Remember to:**
*   Replace `<your-repository-url>` and `<your-repository-name>`.
*   Review the "Prepare Data (Scraping)" section and adjust if you're providing pre-scraped files or if the scraper names are slightly different.
*   Ensure the model names in `.env.example` match what you are actually using and what AI Pipe supports well.
*   Verify the GitHub exclusion list based on the final sizes of your data files.